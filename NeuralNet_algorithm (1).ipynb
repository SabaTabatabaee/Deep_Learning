{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Forward**:\n",
        "\n",
        "For one example $x^{(i)}$:\n",
        "$$z^{(i)} = w^T x^{(i)} + b $$\n",
        "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$\n",
        "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
        "\n",
        "The cost is then computed by summing over all training examples:\n",
        "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$"
      ],
      "metadata": {
        "id": "3EdnpFbd4mKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "\n",
        "    s = 1/(1+np.exp(-z))\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "_W9LZP9O39zS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GVeVA7CO2gvu"
      },
      "outputs": [],
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    w = np.zeros((dim,1))\n",
        "    b = np.float(0)\n",
        "    return w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Back-propagation\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
        "\n",
        "The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ],
      "metadata": {
        "id": "rCG7DGx-3DpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    m = X.shape[1]\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    cost = -1/m * (np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A)))\n",
        "    dw = (1/m) * np.dot(X, (A - Y).T)\n",
        "    db = (1/m) * np.sum(A - Y)\n",
        "    cost = np.squeeze(np.array(cost))\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "JcLyX1CD2r2F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
        "    w = copy.deepcopy(w)\n",
        "    b = copy.deepcopy(b)\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "\n",
        "        grads,cost = propagate(w,b,X,Y)\n",
        "\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        w = w - learning_rate* dw\n",
        "        b = b - learning_rate* db\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            if print_cost:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "NjIyo6Qu29p7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        if A[0, i] > 0.5:\n",
        "            Y_prediction[0,i] = 1\n",
        "        else:\n",
        "             Y_prediction[0,i] = 0\n",
        "\n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "MCWUVVcr3bbb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost):\n",
        "\n",
        "    dim = X_train.shape[0]\n",
        "    w, b = initialize_with_zeros(dim)\n",
        "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "    w = params['w']\n",
        "    b = params['b']\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "\n",
        "    if print_cost:\n",
        "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "DNiAcb0I3mDL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# now we can test our algorith on any data"
      ],
      "metadata": {
        "id": "Gq9uO0YP67cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)\n",
        "##costs = np.squeeze(logistic_regression_model['costs'])\n",
        "##plt.plot(costs)\n",
        "##plt.ylabel('cost')\n",
        "##plt.xlabel('iterations (per hundreds)')\n",
        "##plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
        "##plt.show()\n"
      ],
      "metadata": {
        "id": "whzjoMGI6XiU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learning_rates = [0.01, 0.001, 0.0001]\n",
        "#models = {}\n",
        "\n",
        "#for lr in learning_rates:\n",
        "    #print (\"Training a model with learning rate: \" + str(lr))\n",
        "    #models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
        "    #print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "#for lr in learning_rates:\n",
        "    #plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
        "\n",
        "#plt.ylabel('cost')\n",
        "#plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "#legend = plt.legend(loc='upper center', shadow=True)\n",
        "#frame = legend.get_frame()\n",
        "#frame.set_facecolor('0.90')\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "-98dsCEi7CTZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s6EDCZa7E3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}